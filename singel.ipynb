{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.contrib.rnn import  GRUCell\n",
    "from tensorflow.python.ops.rnn import dynamic_rnn as rnn\n",
    "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
    "from preprocess_data import batch_generator\n",
    "\n",
    "\n",
    "def get_len(dataset):\n",
    "    B, R, C = dataset.shape\n",
    "    dataset_seq_len = []\n",
    "    for i in range(R):\n",
    "        dataset_seq = dataset[:, i, :]\n",
    "        seq_len = []\n",
    "        for x in dataset_seq:\n",
    "            if list(x)[-1]==0:\n",
    "                seq_len.append(list(x).index(0))\n",
    "            else:\n",
    "                seq_len.append(C)\n",
    "        dataset_seq_len.append(seq_len)\n",
    "    return np.array(dataset_seq_len)\n",
    "\n",
    "def get_len_(dataset):\n",
    "    B, C, H = dataset.shape\n",
    "    return np.full((B,C), H)\n",
    "params = {\"NUM_WORDS\": 10000,\n",
    "          \"INDEX_FROM\": 2,\n",
    "          \"SEQUENCE_LENGTH\": 8,\n",
    "          \"NUM_SENTENCE\": 10,\n",
    "          \"EMBEDDING_DIM\": 10,\n",
    "          \"HIDDEN_SIZE\": 15,\n",
    "          \"ATTENTION_SIZE\": 5,\n",
    "          \"KEEP_PROB\": 0.8,\n",
    "          \"BATCH_SIZE\": 25,\n",
    "          \"NUM_EPOCHS\": 10,\n",
    "          \"DELTA\": 0.5,\n",
    "          \"VOCABULARY_SIZE\": 10000}\n",
    "data = batch_generator(batch_size=params[\"BATCH_SIZE\"], vocabulary_size=params[\"VOCABULARY_SIZE\"],\n",
    "                       row=params[\"NUM_EPOCHS\"], column=params[\"SEQUENCE_LENGTH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttentionModel:\n",
    "    def __init__(self, params):\n",
    "        self.NUM_WORDS = params[\"NUM_WORDS\"]\n",
    "        self.INDEX_FROM = params[\"INDEX_FROM\"]\n",
    "        self.SEQUENCE_LENGTH = params[\"SEQUENCE_LENGTH\"]\n",
    "        self.NUM_SENTENCE = params[\"NUM_SENTENCE\"]\n",
    "        self.EMBEDDING_DIM = params[\"EMBEDDING_DIM\"]\n",
    "        self.HIDDEN_SIZE = params[\"HIDDEN_SIZE\"]\n",
    "        self.ATTENTION_SIZE = params[\"ATTENTION_SIZE\"]\n",
    "        self.KEEP_PROB = params[\"KEEP_PROB\"]\n",
    "        self.BATCH_SIZE = params[\"BATCH_SIZE\"]\n",
    "        self.NUM_EPOCHS = params[\"NUM_EPOCHS\"]\n",
    "        self.DELTA = params[\"DELTA\"]\n",
    "        self.VOCABULARY_SIZE = params[\"VOCABULARY_SIZE\"]\n",
    "        self.input_words = tf.placeholder(tf.int32, [self.BATCH_SIZE, self.NUM_SENTENCE, self.SEQUENCE_LENGTH], name=\"input_words\")\n",
    "        self.target_words = tf.placeholder(tf.int32, [self.BATCH_SIZE], name=\"target_words\")\n",
    "        self.seq_len_pl = tf.placeholder(tf.int32, name=\"seq_len_pl\")\n",
    "        self.keep_prob_pl = tf.placeholder(tf.float32)\n",
    "        self.optimizer = None\n",
    "        self.loss = None\n",
    "        self.accuracy = None\n",
    "        self.rnn_outputs = None\n",
    "    def attention(self, inputs):\n",
    "        name = tf.contrib.framework.get_name_scope()\n",
    "        with tf.variable_scope(\"{}/Attention_layer1\".format(name)):\n",
    "            inputs = tf.concat(inputs, 2)\n",
    "            _, S, H = inputs.get_shape()          \n",
    "            # Attention mechanism\n",
    "            W_omega = tf.get_variable(shape=[H, self.ATTENTION_SIZE], initializer=tf.random_normal_initializer(), name=\"w_omega\")\n",
    "            b_omega = tf.get_variable(shape=[self.ATTENTION_SIZE], initializer=tf.constant_initializer(0.0), name=\"b_omega\")\n",
    "            u_omega = tf.get_variable(shape=[self.ATTENTION_SIZE], initializer=tf.random_normal_initializer(), name=\"u_omega\")\n",
    "            H, S = tf.cast(H, tf.int32), tf.cast(S, tf.int32)\n",
    "            u_i = tf.tanh(tf.add(tf.matmul(tf.reshape(inputs, [-1, H]), W_omega), tf.transpose(b_omega)))\n",
    "            uTu = tf.matmul(u_i, tf.reshape(u_omega, [-1, 1]))\n",
    "            exps = tf.reshape(tf.exp(uTu), [-1, S])\n",
    "            alphas = exps / tf.reshape(tf.reduce_mean(exps, 1), [-1, 1])\n",
    "            si = tf.reduce_sum(inputs * tf.reshape(alphas, [-1, S, 1]), 1)\n",
    "\n",
    "        return si\n",
    "\n",
    "    def word2sentence(self, center_words, sequence_length):\n",
    "        with tf.variable_scope(\"word2sentence\"):\n",
    "            embed_matrix = tf.get_variable(shape=[self.VOCABULARY_SIZE, self.EMBEDDING_DIM], initializer=tf.random_normal_initializer(),\n",
    "                                       name=\"embed_matrix\")\n",
    "            embed = tf.nn.embedding_lookup(embed_matrix, center_words, name=\"embed\")\n",
    "            \n",
    "            rnn_outputs, _ = bi_rnn(GRUCell(self.HIDDEN_SIZE), GRUCell(self.HIDDEN_SIZE), inputs=embed,\n",
    "                                    sequence_length=sequence_length, dtype=tf.float32)\n",
    "        attention_word_si = self.attention(rnn_outputs)\n",
    "        return attention_word_si\n",
    "    def sentence2doc(self, attention_word):\n",
    "        with tf.variable_scope(\"sentence2doc\"):\n",
    "            # attention_word = tf.cast(attention_word, tf.int32)         \n",
    "            rnn_outputs, _ = bi_rnn(GRUCell(self.HIDDEN_SIZE), GRUCell(self.HIDDEN_SIZE), inputs=attention_word,\n",
    "                                     dtype=tf.float32)\n",
    "            attention_word_si = self.attention(rnn_outputs)\n",
    "        return attention_word_si\n",
    "    \n",
    "    def optimize(self, attention_word_si):\n",
    "        drop = tf.nn.dropout(attention_word_si, self.keep_prob_pl)\n",
    "        with tf.variable_scope(\"fc\"):\n",
    "            W = tf.get_variable(shape=[drop.get_shape()[1].value, 5], initializer=tf.random_normal_initializer(stddev=0.1), name=\"fc_weight\")\n",
    "            b = tf.get_variable(shape=[5], initializer=tf.constant_initializer(0.0), name=\"fc_bias\")\n",
    "            y_hat = tf.nn.xw_plus_b(drop, W, b)\n",
    "            y_hat = tf.squeeze(y_hat)\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            target_words = tf.one_hot(self.target_words-1, depth=5)\n",
    "            self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=target_words))\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(self.loss)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_hat)), target_words), tf.float32))\n",
    "\n",
    "    def train(self, data):\n",
    "        attention_word = []\n",
    "        with tf.variable_scope(\"word2sentence\") as scope:\n",
    "            for i in range(self.NUM_SENTENCE):\n",
    "                center_words = self.input_words[:, i, :]\n",
    "                sequence_length = self.seq_len_pl[i, :]\n",
    "                attention_word_si = self.word2sentence(center_words, sequence_length)\n",
    "                attention_word_si = tf.expand_dims(attention_word_si, axis=1)\n",
    "                attention_word.append(attention_word_si)\n",
    "                scope.reuse_variables()\n",
    "        a = attention_word\n",
    "        attention_word = tf.concat(attention_word, axis=1)\n",
    "        \n",
    "        s = self.sentence2doc(attention_word)\n",
    "        self.optimize(s)\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print(\"Start Training...\")\n",
    "            for epoch in range(1):\n",
    "                loss_train = 0\n",
    "                loss_test = 0\n",
    "                accuracy_train = 0\n",
    "                accuracy_test = 0\n",
    "                print(\"epoch: {}\\t\".format(epoch), end=\"\")\n",
    "                num_batches = 1\n",
    "                for b in range(num_batches):\n",
    "                    x_batch, y_batch = data.__next__()\n",
    "                    seq_len = get_len(x_batch)\n",
    "                    loss_tr, acc, _ = sess.run([self.loss, self.accuracy, self.optimizer],\n",
    "                                               feed_dict={self.input_words: x_batch, self.target_words: y_batch,\n",
    "                                                          self.seq_len_pl: seq_len, self.keep_prob_pl: 0.8})\n",
    "                    accuracy_train += acc\n",
    "                    loss_train = loss_tr * self.DELTA + loss_train * (1 - self.DELTA)\n",
    "                accuracy_train /= num_batches\n",
    "                print(accuracy_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "epoch: 0\t0.663999974728\n"
     ]
    }
   ],
   "source": [
    "model = AttentionModel(params)\n",
    "model.train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
